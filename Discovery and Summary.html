<!DOCTYPE html>

<html>
<style>
    #goToTop {
        display: none;
        position: fixed;
        bottom: 20px;
        right: 30px;
        z-index: 99;
        border: none;
        outline: none;
        background-color: rgb(51, 153, 255, 1.0);
        color: white;
        cursor: pointer;
        padding: 15px;
        border-radius: 10px;
        font-size: 20px;
        font-weight: bold;
    }
    #goToTop:hover {
        background-color: rgb(255, 153, 51, 1.0);
    }

    body{font-size: 20px;
         background-color: #DBF9FC}

    hr {
        border-color: black;
        }

    span {
        color: rgb(204, 0, 0, 1.0);
        font-weight: bold;
    }


    }
</style>

<head>
    <title>Discovery and Summary</title>    
</head>

<body>
    <button onclick="topFunction()" id="goToTop" title="Go to top">Top</button>
    <script>
let gototop = document.getElementById("goToTop");

window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 700 || document.documentElement.scrollTop > 700) {
    gototop.style.display = "block";
  } else {
    gototop.style.display = "none";
  }
}

function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
    </script>

    <h2>CS1102 - Course Project - 2023/2024 Semester B</h2>
    <h3>FOK Tsz Ying (56574045) SUN Ying Kwing (57140726)</h3>
    <h3>ZHANG Zhen (57846711) CHAU Yin Tau (57140820)</h3><hr>

    <h3>Go back to <a href="index.html">Home</a> or Revisit:</h3>
     <p><a href="Architecture of LLMs.html">Architecture</a></p>
     <p><a href="Pros and Cons of LLMs.html">Pros & Cons</a></p>
     <p><a href="Limitations.html">Limitations</a></p><hr>
    
    <h3>Jump to:</h3>
    <p>1. <a href="#Discovery">Discovery</a></p>
    <p>2. <a href="#Summary">Summary</a></p>
    <p>3. <a href="#Future">Future of LLMs</a></p>
    <p>4. <a href="#Reference">Reference List</a></p>
    <hr>

    <h1 id="Discovery">Discovery</h1>

     <h2>1. Mechanism</h2>
     <p>In this project, we informed ourselves of the <b>detailed mechanism of LLMs</b>, which we normally would not have the chance of knowing. Namely, we learnt about the <b>methodologies and technologies</b> responsible for LLMs’ competencies, including <span>deep learning</span>, <span>deep neural network</span> and <span>natural language processing</span>.</p>

     <h2>2. Wide Range of Applications</h2>
     <p>As university students, our involvement with LLMs does not usually go beyond using ChatGPT to help with our studies. Therefore, it was eye-opening to learn about <b>LLMs’ potential applications in diverse professional fields</b>, such as <span>education</span>, <span>healthcare</span>, <span>finance</span> and the <span>legal industry</span> etc.</p>

     <h2>3. Frequent Hallucinations</h2>
     <p>It came as a shock to us that LLMs <b>regularly give out deceiving or erroneous answers</b> known as '<i>hallucinations</i>' and even present them in an <b>assertive and convincing tone</b>. Therefore, it is important to check the validity and truthfulness</b> of LLMs’ outputs to avoid being misled.

     <h2>4. High Energy Usage</h2>
     <p>As users, we only have to type in prompts and enjoy quick and detailed answer provided by LLMs. Therefore, it was surprising to discover the <b>high energy demand behind their trainings</b> and the corresponding <b>environmental issues</b>.</p><hr>

    <h1 id="Summary">Summary</h1>

    <h2>1. Definition</h2>
    <p>A large language model (LLM) is an <b>artificial intelligence (AI) language model</b> that can learn to <b>understand and interpret human languages</b> via training with extensive amounts of data with the use of deep learning. In doing so, LLMs are ultimately capable of <b>complex language processing</b>, such as text generation, translation and sentiment analysis etc.</p>

    <h2>2. Architecture</h2>
    <p>LLMs gain their abilities through <b>deep learning</b>, which involves the employment of a multi-layered <b>deep neural network</b>. Inside the network, interconnected nodes within the <b>hidden layers</b> help LLMs perform analysis, computation or transformation on input data. Meanwhile, <b>Natural Language Processing (NLP)</b> is responsible for the understanding of human-language inputs and the generation of human-readable outputs.</p>

    <h4>Figure 1</h4>
    <p>Example of a deep neural network</p>
    <img src="Images/InputHiddenOutputlayer.jpg" alt="Example of a deep neural network" width="500px"></br>
    <p>Note. From File:Example of a deep neural network.png, by BrunelloN, 2021, Wikimedia Commons (<a href="https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png" target="_blank">https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png</a>).</p>

    <h2>3. Advantages of LLMs</h2>
    <ol>
        <li><b>Cross-Domain Versatility:</b> Easily applied across domains and useful for personalized content recommendations and summarizations</li><br>
        <li><b>Enhanced Security Applications:</b> Identification of cybercrimes and protection of digital identities via interpretation and analysis of the visual data</li><br>
        <li><b>Multilingual Proficiency:</b></li><br>
          <ul>
             <li>Assist cross-cultural exchanges by overcoming language differences</li><br>
             <li>Facilitate democratization of information</li><br>
             <li>Globalization of businesses</li>
          </ul><br>
         <li><b>Revolutionizing Education through Personalization and Accessibility:</b></li><br>
          <ul>
             <li>Offer highly personalized learning experiences</li><br>
             <li>Create deeper engagements with interactive dialogues and feedback mechanisms</li><br>
             <li>Make education accessible regardless of geographical or financial barriers</li>
          </ul>
    </ol>

    <h2>4. Disadvantages of LLMs</h2>
    <ol>
        <li><b>Data Bias and Misinformation:</b>  Reinforce Stereotypes & Perpetuate Inequalities</li><br>
        <li><b>Risk of Cognitive Complacency:</b> Reliance on LLMs for problem-solving may hinder critical thinking and creativity</li><br>
        <li><b>Environmental Issues:</b> High energy consumption and carbon emissions from trainings of large LLMs</li><br>
        <li><b>Inaccurate/Ineffective Entity Matching</b></li>
    </ol>

    <h2>5. Limitations of LLMs</h2>
    <ol>
        <li><b>Reliance on Training:</b> processing data ncountered outside their training may often lead to inaccurate or illogical outputs</li><br>
        <li><b>Limited Input & Output Length</b></li>
    </ol><hr>

    <h1 id="Future">Future of LLMs</h1>
    <h4>In the future, LLMs will continue to advance and become even better in text processing and generation. In addition, LLMs may be able to process visual and audio inputs for a better understanding of context so that they can be more effectively applied across different industries, such as:</h4>

    <h3>1. Healthcare</h3>
     <ul>
         <li><b>Allow patient communication unrestrained by time or location</b></li><br>
         <li><b>Optimize documentation process by organizing unstructured data</b></li><br>
         <li><b>Assist with public health initiatives via trend recognition and analysis</b></li>
     </ul>
    <p>(Clusmann et al., 2023)</p>

    <h3>2. Finance</h3>
     <ul>
         <li><b>Time Series Forecasting:</b> Predict future price trends using historical data</li><br>
         <li><b>Portfolio Optimization:</b> Support investment decisions across various assets</li><br>
         <li><b>Insolvency Forecasting:</b> Analyze financial reports/ news sources to identify early symptoms of financial distress</li>
     </ul>
    <p>(Zhao et al., 2023)</p>

    <h3>3. Legal</h3>
     <ul>
         <li><b>Contract Review/ Drafting:</b> Identify essential clauses, errors, potential risks etc.; draft brand new contracts that follow specific requirements/formats</li><br>
         <li><b>Litigation Analysis:</b> Predict success rates, risks and costs to form effective strategies</li><br>
         <li><b>Legal research:</b> Provide relevant information based on prompts given and answer questions based on findings</li>
     </ul>
    <p>(Sydorenko, 2023)</p>

    <h4>Furthermore, according to Toews (2024), here are three directions of development for LLMs that can mitigate their current flaws:</h4>

    <h3>1. Self-Training</h3>
    <p>Currently, LLMs rely on information consumed during their training to produce outputs. However, Toews points out that it could be possible that LLMs <b>write new content themselves</b> based on knowledge already obtained and use this content as <b>new training data</b> to '<i>fine-tune</i>' itself. One may understandably question the logic of this approach as it seems like a useless cycle. However, a more accurate description would be doing self-reflection or self-questioning to gain new insights. Aside from self-improvement, LLMs being able to generate its own training data is also important as available text training data may soon be exhausted entirely.</p>

    <h3>2. Automated Fact-Checking</h3>
    <p>Current efforts are being made to help improve the accuracy of LLMs’ outputs and avoid hallucinations. Specifically, developers wish to give LLMs the ability to <b>gather information from external sources</b>. This allows LLMs access to the most accurate and relevant information available online. Furthermore, developers hope to let LLMs <b>provide references and citations</b> for their answers. Not only does this increase validity, it also allows users to check if the sources provided are reliable.</p>

    <h3>3. Sparse Expert Models</h3>
    <p>Sparse expert models have a <b>different architecture</b> than many popular ‘dense’ LLMs nowadays. To elaborate, rather than activating all of its parameters everytime it is given a prompt, a sparse expert model only calls upon the ones that are the <b>most relevant and helpful</b>. Because of this, sparse expert models can be <b>larger in scale but take less energy to train</b>. Moreover, they <b>outperform traditional LLMs while requiring much less computation</b>. Even though sparse expert models have a more technically complex structure and the concept of it is not yet well-known, they hold <b>massive potential</b> for the future of LLMs.</p><hr>

    <h1 id="Reference">Reference List</h1>
    <ol>
        <li>Al-Hasan, T. M., Sayed, A. N., Bensaali, F., Himeur, Y., Varlamis, I., & Dimitrakopoulos, G. (2024). From Traditional Recommender Systems to GPT-Based Chatbots: A Survey of Recent Developments and Future Directions. <i>Big Data and Cognitive Computing, 8</i>(4). <a href="https://doi.org/10.3390/bdcc8040036" target="_blank">https://doi.org/10.3390/bdcc8040036</a></li><br>

        <li>Bak, M., & Chin, J. (2024). The potential and limitations of large language models in identification of the states of motivations for facilitating health behavior change. <i>Journal of the American Medical Informatics Association.</i> <a href="https://doi.org/10.1093/jamia/ocae057" target="_blank">https://doi.org/10.1093/jamia/ocae057</a></li><br>

        <li>BrunelloN. (2021, August 12). <i>File:Example_of_a_deep_neural_network.png.</i> Wikimedia Commons. <a href="https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png" target="_blank">https://commons.wikimedia.org/wiki/File:Example_of_a_deep_neural_network.png</a></li><br>

        <li>Cahyawijaya , S., Lovenia, H., & Fung, P. (2024). <i>LLMs Are Few-Shot In-Context Low-Resource Language Learners.</i> <a href="https://doi.org/10.48550/arXiv.2403.16512" target="_blank">https://doi.org/10.48550/arXiv.2403.16512</a></li><br>

        <li>Chaikiatsri, P., & Rattanasopon, S. (2024). <i>Evaluating the Multilingual Differences of ChatGPT and Google Gemini on the MMLU Dataset Translated into Thai.</i> <a href="https://doi.org/10.31219/osf.io/smkfu" target="_blank">https://doi.org/10.31219/osf.io/smkfu</a></li><br>

        <li>Clusmann, J., Kolbinger, F. R., Muti, H. S., Carrero, Z. I., Eckardt, J.-N., Laleh, N. G., Löffler, C. M. L., Schwarzkopf, S.-C., Unger, M., Veldhuizen, G. P., Wagner, S. J., & Kather, J. N. (2023, October 10). <i>The future landscape of large language models in medicine.</i> Nature News. <a href="https://www.nature.com/articles/s43856-023-00370-1" target="_blank">https://www.nature.com/articles/s43856-023-00370-1</a></li><br>

        <li>Daws, R. (2024, January 24). <i>NCSC: AI to significantly boost cyber threats over next two years.</i> AI News. <a href="https://www.artificialintelligence-news.com/2024/01/24/ncsc-ai-significantly-boost-cyber-threats-next-two-years/" target="_blank">https://www.artificialintelligence-news.com/2024/01/24/ncsc-ai-significantly-boost-cyber-threats-next-two-years/</a></li><br>

        <li>Ding, H., Dai, C., Wu, Y., Ma, W., & Zhou, H. (2024). Setem: Self-ensemble training with Pre-trained Language Models for Entity Matching. <i>Knowledge-Based Systems, 293.</i> <a href="https://doi.org/10.1016/j.knosys.2024.111708" target="_blank">https://doi.org/10.1016/j.knosys.2024.111708</a></li><br>

        <li>Fazackerley, A. (2023, March 19). <i>AI makes plagiarism harder to detect, argue academics – in paper written by chatbot.</i> The Guardian. <a href="https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot" target="_blank">https://www.theguardian.com/technology/2023/mar/19/ai-makes-plagiarism-harder-to-detect-argue-academics-in-paper-written-by-chatbot</a></li><br>

        <li><i>GPT-4 and GPT-4 Turbo.</i> OpenAI. (n.d.). <a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" target="_blank">https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo</a></li><br>

        <li>Gu, Z., Sun, X., Lian, F., Kang, Z., Xu, C., & Fan, J. (2024). Dingo: Towards Diverse and Fine-Grained Instruction-Following Evaluation. <i>Proceedings of the AAAI Conference on Artificial Intelligence, 38</i>(16), 18108–18116. <a href="https://doi.org/10.1609/aaai.v38i16.29768" target="_blank">https://doi.org/10.1609/aaai.v38i16.29768</a></li><br>

        <li><i>Hidden layer.</i> DeepAI. (2019, May 17). <a href="https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning" target="_blank">https://deepai.org/machine-learning-glossary-and-terms/hidden-layer-machine-learning</a></li><br>

        <li><i>Hidden layer.</i> DevX. (2023, December 19). <a href="https://www.devx.com/terms/hidden-layer/" target="_blank">https://www.devx.com/terms/hidden-layer/</a></li><br>

        <li><i>Introducing Llama: A foundational, 65-billion-parameter language model.</i> Meta. (2023, February 24). <a href=" https://ai.meta.com/blog/large-language-model-llama-meta-ai/" target="_blank"> https://ai.meta.com/blog/large-language-model-llama-meta-ai/</a></li><br>

        <li>Kasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., … Kasneci, G. (2023). Chatgpt for good? On opportunities and challenges of large language models for education. <i>Learning and Individual Differences, 103.</i> <a href="https://doi.org/10.1016/j.lindif.2023.102274" target="_blank">https://doi.org/10.1016/j.lindif.2023.102274</a></li><br>

        <li>Kerner, S. M. (2023, September 13). <i>What are large language models?: Definition from TechTarget.</i> WhatIs. <a href="https://www.techtarget.com/whatis/definition/large-language-model-LLM" target="_blank">https://www.techtarget.com/whatis/definition/large-language-model-LLM</a></li><br>

        <li>Kolbert, E. (2024, March 9). <i>The obscene energy demands of A.I.</i> The New Yorker. <a href="https://www.newyorker.com/news/daily-comment/the-obscene-energy-demands-of-ai" target="_blank">https://www.newyorker.com/news/daily-comment/the-obscene-energy-demands-of-ai</a></li><br>

        <li>Lewis, M., & Mitchell, M. (2024). <i>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models.</i> <a href="https://doi.org/10.48550/arXiv.2402.08955" target="_blank">https://doi.org/10.48550/arXiv.2402.08955</a></li><br>

        <li>Marr, B. (2024, February 20). <i>A short history of ChatGPT: How we got to where we are today.</i> Forbes. <a href="https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/?sh=5df9e04b674f" target="_blank">https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/?sh=5df9e04b674f</a></li><br>

        <li><i>Natural Language Processing (NLP) - A complete guide.</i> DeepLearning.AI. (2023, January 11). <a href="https://www.deeplearning.ai/resources/natural-language-processing/" target="_blank">https://www.deeplearning.ai/resources/natural-language-processing/</a></li><br>

        <li>Nguyen, B. (2024, March 6). <i>ChatGPT is bad at following copyright law, researchers say.</i> Quartz. <a href="https://qz.com/openai-chatgpt-anthropic-claude-copyright-law-violation-1851311580" target="_blank">https://qz.com/openai-chatgpt-anthropic-claude-copyright-law-violation-1851311580</a></li><br>

        <li>Ognjanovski, G. (2020, June 7). <i>Everything you need to know about neural networks and backpropagation-machine learning made easy and fun.</i> Medium. <a href="https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a" target="_blank">https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a</a></li><br>

        <li>Park, K. (2023, May 2). <i>Samsung bans use of generative AI tools like ChatGPT after April internal data leak.</i> TechCrunch. <a href="https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/" target="_blank">https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/</a></li><br>

        <li>Pieuchon, N. A. de, Daoud, A., Jerzak, C. T., Johansson, M., & Johansson, R. (2024). <i>Can Large Language Models (or Humans) Distill Text?</i> <a href="https://doi.org/10.48550/arXiv.2403.16584" target="_blank">https://doi.org/10.48550/arXiv.2403.16584</a></li><br>

        <li>Proudfoot, O. (2024, March 22). <i>What is Claude AI, and how does it compare to ChatGPT?.</i> Pluralsight. <a href="https://www.pluralsight.com/resources/blog/data/what-is-claude-ai" target="_blank">https://www.pluralsight.com/resources/blog/data/what-is-claude-ai</a></li><br>

        <li>Sallam, M. (2023). <i>The Utility of ChatGPT as an Example of Large Language Models in Healthcare Education, Research and Practice: Systematic Review on the Future Perspectives and Potential Limitations.</i> <a href="https://doi.org/10.1101/2023.02.19.23286155" target="_blank">https://doi.org/10.1101/2023.02.19.23286155</a></li><br>

        <li>Shenwai, D. S. (2023, June 27). <i>8 potentially surprising things to know about large language models LLMs.</i> MarkTechPost. <a href="https://www.marktechpost.com/2023/06/27/8-potentially-surprising-things-to-know-about-large-language-models-llms/" target="_blank">https://www.marktechpost.com/2023/06/27/8-potentially-surprising-things-to-know-about-large-language-models-llms/</a></li><br>

        <li>Sydorenko, P. (2023, August 22). <i>Top 5 applications of large language models (LLMs) in legal practice.</i> Medium. <a href="https://medium.com/jurdep/top-5-applications-of-large-language-models-llms-in-legal-practice-d29cde9c38ef" target="_blank">https://medium.com/jurdep/top-5-applications-of-large-language-models-llms-in-legal-practice-d29cde9c38ef</a></li><br>

        <li>Toews, R. (2024, February 20). <i>The next generation of large language models.</i> Forbes. <a href="https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=5b55b0af18db" target="_blank">https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=5b55b0af18db</a></li><br>

        <li>Trad, F., & Chehab, A. (2024). <i>Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications.</i> <a href="https://doi.org/10.48550/arXiv.2403.17787" target="_blank">https://doi.org/10.48550/arXiv.2403.17787</a></li><br>

        <li><i>What are large language models (LLMs)?.</i> IBM. (n.d.-a). <a href="https://www.ibm.com/topics/large-language-models" target="_blank">https://www.ibm.com/topics/large-language-models</a></li><br>

        <li><i>What is deep learning?.</i> IBM. (n.d.-b). <a href="https://www.ibm.com/topics/deep-learning" target="_blank">https://www.ibm.com/topics/deep-learning</a></li><br>

        <li><i>What is the transformer architecture and how does it work?.</i> Datagen. (2023, May 22). <a href="https://datagen.tech/guides/computer-vision/transformer-architecture/" target="_blank">https://datagen.tech/guides/computer-vision/transformer-architecture/</a></li><br>

        <li>Zhao, H., Liu, Z., Wu, Z., Li, Y., Yang, T., Shu, P., Xu, S., Dai, H., Zhao, L., Mai, G., Liu, N., & Liu, T. (2024). <i>Revolutionizing Finance with LLMs: An Overview of Applications and Insights.</i> <a href="hhttps://doi.org/10.48550/arXiv.2401.11641" target="_blank">https://doi.org/10.48550/arXiv.2401.11641</a></li><br>
    </ol>

</body>
</html>